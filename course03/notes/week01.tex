\section{Week 1 - ML Strategy (1)}
\subsection{Introduction to ML Strategy}
\subsubsection{Why ML Strategy}
To improve your deep learning algorithm, you may end up with lot of ideas for improvement like:
\begin{itemize}
    \item Collect more data
    \item Collect more diverse training set
    \item Train algorithm longer with gradient descent
    \item Try Adam instead of gradient descent
    \item Try bigger network
    \item Try smaller network
    \item Try dropout
    \item Add $L_2$ regularization
    \item Network architecture: Activation functions, \# hidden units
\end{itemize}
\subsubsection{Orthogonalization}
Orthogonalization or orthogonality is a system design property that assures that modifying an instruction or a component of an algorithm will not create or propagate side effects to other components of the system. It becomes easier to verify the algorithms independently from one another, it reduces testing and development time.

When a supervised learning system is design, these are the 4 assumptions that needs to be true and orthogonal.
\begin{enumerate}
    \item Fit training set well in cost function
    \begin{itemize}
        \item If it doesn't fit well, the use of a bigger neural network or switching to a better optimization algorithm might help.
    \end{itemize}
    \item Fit development set well on cost function
    \begin{itemize}
        \item If it doesn't fit well, regularization or using bigger training set might help.
    \end{itemize}
    \item Fit test set well on cost function
    \begin{itemize}
        \item If it doesn't fit well, the use of a bigger development set might help
    \end{itemize}
    \item Performs well in real world
    \begin{itemize}
        \item If it doesn’t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing.
    \end{itemize}
\end{enumerate}

\subsection{Setting up your goal}
\subsubsection{Single number evaluation metric}
\subsubsection{Satisficing and Optimizing metric} If there are multiple things you care about, e.g.: 
\begin{itemize}
    \item the optimizing metric that you want to do as well as possible on
    \item one or more as satisfying metrics were you'll be satisfied
\end{itemize}
Almost it does better than some threshold you can now have an almost automatic way of quickly looking at multiple core size and picking the best one. 
These evaluation matrix must be evaluated or calculated on a \textit{training} set or a \textit{development} set or maybe on the \textit{test} set.

\subsubsection{Train/dev/test distributions} Setting up the training, development and test sets have a huge impact on productivity. It is important to choose the development and test sets from the \textbf{same distribution} and it must be \textbf{taken randomly} from all the data.
\textbf{Guideline:} Choose a development set and test set to reflect data you expect to get in the future and consider important to do well.

\subsubsection{Size of the dev and test sets} 

\subsubsection{When to change dev/test sets and metrics}

\subsection{Comparing to human-level performance}
\subsubsection{Why human-level performance?}

\subsubsection{Avoidable bias} By knowing what the human-level performance is, it is possible to tell when a training set is performing well or not.

Example: Cat vs Non-Cat
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
  & \multicolumn{2}{c}{Classification error (\%)} \\ 
 \hline
  & Scenario A & Scenario B \\ 
 \hline
 Humans & 1 & 7.5 \\
 \hline
 Training error & 8 & 8 \\ 
 \hline
 Development error & 10 & 10 \\
 \hline
\end{tabular}
\end{center}
In this case, the human level error as a proxy for Bayes error since humans are good to identify images. If you want to improve the performance of the training set but you can’t do better than the Bayes error otherwise the training set is overfitting. By knowing the Bayes error, it is easier to focus on whether bias or variance avoidance tactics will improve the performance of the model.
\begin{description}
  \item[Scenario A] There is a 7\% gap between the performance of the training set and the human level error. It means that the algorithm isn’t fitting well with the training set since the target is around 1\%. To resolve the issue, we use bias reduction technique such as training a bigger neural network or running the training set longer.
  \item[Scenario B] The training set is doing good since there is only a 0.5\% difference with the human level error. The difference between the training set and the human level error is called avoidable bias. The focus here is to reduce the variance since the difference between the training error and the development error is 2\%. To resolve the issue, we use variance reduction technique such as regularization or have a bigger training set.
\end{description}

\subsubsection{Surpassing human-level performance} Example1: Classification task
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
  & \multicolumn{2}{c}{Classification error (\%)} \\ 
 \hline
  & Scenario A & Scenario B \\ 
 \hline
 Team of Humans & 0.5 & 0.5 \\
 \hline
 One Human & 1.0 & 1 \\
 \hline
 Training error & 0.6 & 0.3 \\ 
 \hline
 Development error & 0.8 & 0.4 \\
 \hline
\end{tabular}
\end{center}
\begin{description}
  \item[Scenario A] In this case, the Bayes error is 0.5\%, therefore the available bias is 0.1\% et the variance is 0.2\%. 
  \item[Scenario B] In this case, there is not enough information to know if bias reduction or variance reduction has to be done on the algorithm. It doesn’t mean that the model cannot be improve, it means that the conventional ways to know if bias reduction or variance reduction are not working in this case.
\end{description}
There are many problems where machine learning significantly surpasses human-level performance, especially with structured data:
\begin{itemize}
    \item Online advertising
    \item Product recommendations
    \item Logistics (predicting transit time)
    \item Loan approvals
\end{itemize}

\subsubsection{Improving your model performance}

\subsection{Machine Learning flight simulator}

\subsubsection*{QUIZ - Bird recognition in the city of Peacetopia (case study)}
\textbf{1.} Problem Statement: This example is adapted from a real production application, but with details disguised to protect confidentiality.

You are a famous researcher in the City of Peacetopia. The people of Peacetopia have a common characteristic: they are afraid of birds. To save them, you have \textbf{to build an algorithm that will detect any bird flying over Peacetopia} and alert the population.

The City Council gives you a dataset of 10,000,000 images of the sky above Peacetopia, taken from the city’s security cameras. They are labelled:
\begin{itemize}
    \item y = 0: There is no bird on the image
    \item y = 1: There is a bird on the image
\end{itemize}
Your goal is to build an algorithm able to classify new images taken by security cameras from Peacetopia. There are a lot of decisions to make:
\begin{itemize}
    \item What is the evaluation metric?
    \item How do you structure your data into train/dev/test sets?
\end{itemize}
\texttt{Metric of success}
The City Council tells you that they want an algorithm that
\begin{enumerate}
    \item Has high accuracy
    \item Runs quickly and takes only a short time to classify a new image.
    \item Can fit in a small amount of memory, so that it can run in a small processor that the city will attach to many different security cameras.
\end{enumerate}
\texttt{Note:} Having three evaluation metrics makes it harder for you to quickly choose between two different algorithms, and will slow down the speed with which your team can iterate. True/False?
\begin{itemize}
    \item True (X)
    \item False
\end{itemize}
\textbf{2.} After further discussions, the city narrows down its criteria to:
\begin{itemize}
    \item "We need an algorithm that can let us know a bird is flying over Peacetopia as accurately as possible."
    \item "We want the trained model to take no more than 10sec to classify a new image.”
    \item “We want the model to fit in 10MB of memory.”
\end{itemize}
If you had the three following models, which one would you choose?
\begin{itemize}
    \item Test Accuracy: 97\%, Runtime: 1 sec, Memory size: 3MB
    \item Test Accuracy: 99\%, Runtime: 13 sec, Memory size: 9MB
    \item Test Accuracy: 97\%, Runtime: 3 sec, Memory size: 2MB
    \item Test Accuracy: 98\%, Runtime: 9 sec, Memory size: 9MB (X)
\end{itemize}
\textbf{3.} Based on the city’s requests, which of the following would you say is true?
\begin{itemize}
    \item Accuracy is an optimizing metric; running time and memory size are a satisficing metrics. (X)
    \item Accuracy is a satisficing metric; running time and memory size are an optimizing metric.
    \item Accuracy, running time and memory size are all optimizing metrics because you want to do well on all three.
    \item Accuracy, running time and memory size are all satisficing metrics because you have to do sufficiently well on all three for your system to be acceptable.
\end{itemize}
\textbf{4.} \texttt{Structuring your data:} Before implementing your algorithm, you need to split your data into train/dev/test sets. Which of these do you think is the best choice?
\begin{itemize}
    \item Train: 6,000,000; Dev: 3,000,000; Test: 1,000,000
    \item Train: 6,000,000; Dev: 1,000,000; Test: 3,000,000
    \item Train: 3,333,334; Dev: 3,333,333; Test: 3,333,333
    \item Train: 9,500,000; Dev: 250,000; Test: 250,000 (X)
\end{itemize}
\textbf{5.} After setting up your train/dev/test sets, the City Council comes across another 1,000,000 images, called the “citizens’ data”. Apparently the citizens of Peacetopia are so scared of birds that they volunteered to take pictures of the sky and label them, thus contributing these additional 1,000,000 images. These images are different from the distribution of images the City Council had originally given you, but you think it could help your algorithm.

You should not add the citizens’ data to the training set, because this will cause the training and dev/test set distributions to become different, thus hurting dev and test set performance. True/False?
\begin{itemize}
    \item True
    \item False (X)
\end{itemize}
\textbf{6.} One member of the City Council knows a little about machine learning, and thinks you should add the 1,000,000 citizens’ data images to the test set. You object because:
\begin{itemize}
    \item This would cause the dev and test set distributions to become different. This is a bad idea because you’re not aiming where you want to hit. (X)
    \item The 1,000,000 citizens’ data images do not have a consistent x-->y mapping as the rest of the data (similar to the New York City/Detroit housing prices example from lecture).
    \item The test set no longer reflects the distribution of data (security cameras) you most care about. (X)
    \item A bigger test set will slow down the speed of iterating because of the computational expense of evaluating models on the test set.
\end{itemize}
\textbf{7.} You train a system, and its errors are as follows (error = 100\%-Accuracy):
\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 Training set error & 4.0\% \\
 Dev set error & 4.5\% \\
 \hline
\end{tabular}
\end{center}

This suggests that one good avenue for improving performance is to train a bigger network so as to drive down the 4.0\% training error. Do you agree?
\begin{itemize}
    \item Yes, because having 4.0\% training error shows you have high bias. (X 1)
    \item Yes, because this shows your bias is higher than your variance.
    \item No, because this shows your variance is higher than your bias.
    \item No, because there is insufficient information to tell. (X 2)
\end{itemize}
\textbf{8.} You ask a few people to label the dataset so as to find out what is human-level performance. You find the following levels of accuracy:
\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 Bird watching expert \#1 & 0.3\% error \\
 Bird watching expert \#2 & 0.5\% error \\
 Normal person \#1 (not a bird watching expert) & 1.0\% error \\
 Normal person \#2 (not a bird watching expert) & 1.2\% error \\
 \hline
\end{tabular}
\end{center}
If your goal is to have “human-level performance” be a proxy (or estimate) for Bayes error, how would you define “human-level performance”?
\begin{itemize}
    \item 0.0\% (because it is impossible to do better than this)
    \item 0.3\% (accuracy of expert \#1) (X)
    \item 0.4\% (average of 0.3 and 0.5)
    \item 0.75\% (average of all four numbers above)
\end{itemize}
\textbf{9.} Which of the following statements do you agree with?
\begin{itemize}
    \item A learning algorithm’s performance can be better than human-level performance but it can never be better than Bayes error. (X 2)
    \item A learning algorithm’s performance can never be better than human-level performance but it can be better than Bayes error.
    \item A learning algorithm’s performance can never be better than human-level performance nor better than Bayes error.
    \item A learning algorithm’s performance can be better than human-level performance and better than Bayes error. (X 1)
\end{itemize}
\textbf{10.} You find that a team of ornithologists debating and discussing an image gets an even better 0.1\% performance, so you define that as “human-level performance.” After working further on your algorithm, you end up with the following:
\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 Human-level performance & 0.1\% \\
Training set error & 2.0\% \\
Dev set error & 2.1\% \\
 \hline
\end{tabular}
\end{center}
Based on the evidence you have, which two of the following four options seem the most promising to try? (Check two options.)
\begin{itemize}
    \item Train a bigger model to try to do better on the training set. (X)
    \item Get a bigger training set to reduce variance.
    \item Try decreasing regularization. (X)
    \item Try increasing regularization.
\end{itemize}
\textbf{11.} You also evaluate your model on the test set, and find the following:
\begin{center}
\begin{tabular}{ |c|c| }
 \hline
Human-level performance & 0.1\% \\
Training set error & 2.0\% \\
Dev set error & 2.1\% \\
Test set error & 7.0\% \\
 \hline
\end{tabular}
\end{center}
What does this mean? (Check the two best options.)
\begin{itemize}
    \item You should get a bigger test set.
    \item You should try to get a bigger dev set. (X)
    \item You have overfit to the dev set. (X)
    \item You have underfit to the dev set.
\end{itemize}
\textbf{12.} After working on this project for a year, you finally achieve:
\begin{center}
\begin{tabular}{ |c|c| }
 \hline
Human-level performance & 0.10\% \\
Training set error & 0.05\% \\
Dev set error & 0.05\% \\
 \hline
\end{tabular}
\end{center}
What can you conclude? (Check all that apply.)
\begin{itemize}
    \item If the test set is big enough for the 0.05\% error estimate to be accurate, this implies Bayes error is $\leq$ 0.05 (X)
    \item It is now harder to measure avoidable bias, thus progress will be slower going forward. (X)
    \item With only 0.09\% further progress to make, you should quickly be able to close the remaining gap to 0\%
    \item This is a statistical anomaly (or must be the result of statistical noise) since it should not be possible to surpass human-level performance.
\end{itemize}
\textbf{13.} It turns out Peacetopia has hired one of your competitors to build a system as well. Your system and your competitor both deliver systems with about the same running time and memory size. However, your system has higher accuracy! However, when Peacetopia tries out your and your competitor’s systems, they conclude they actually like your competitor’s system better, because even though you have higher overall accuracy, you have more false negatives (failing to raise an alarm when a bird is in the air). What should you do?
\begin{itemize}
    \item Look at all the models you’ve developed during the development process and find the one with the lowest false negative error rate.
    \item Ask your team to take into account both accuracy and false negative rate during development. (X 1)
    \item Rethink the appropriate metric for this task, and ask your team to tune to the new metric.
    \item Pick false negative rate as the new metric, and use this new metric to drive all further development. (X 2)
\end{itemize}
\textbf{14.} You’ve handily beaten your competitor, and your system is now deployed in Peacetopia and is protecting the citizens from birds! But over the last few months, a new species of bird has been slowly migrating into the area, so the performance of your system slowly degrades because your data is being tested on a new type of data.

You have only 1,000 images of the new species of bird. The city expects a better system from you within the next 3 months. Which of these should you do first?
\begin{itemize}
    \item Use the data you have to define a new evaluation metric (using a new dev/test set) taking into account the new species, and use that to drive further progress for your team.
    \item Put the 1,000 images into the training set so as to try to do better on these birds.
    \item Try data augmentation/data synthesis to get more images of the new type of bird. (X 1)
    \item Add the 1,000 images into your dataset and reshuffle into a new train/dev/test split. (X 2)
\end{itemize}
\textbf{15.} The City Council thinks that having more Cats in the city would help scare off birds. They are so happy with your work on the Bird detector that they also hire you to build a Cat detector. (Wow Cat detectors are just incredibly useful aren’t they.) Because of years of working on Cat detectors, you have such a huge dataset of 100,000,000 cat images that training on this data takes about two weeks. Which of the statements do you agree with? (Check all that agree.)
\begin{itemize}
    \item Buying faster computers could speed up your teams’ iteration speed and thus your team’s productivity. (X)
    \item Needing two weeks to train will limit the speed at which you can iterate. (X)
    \item Having built a good Bird detector, you should be able to take the same model and hyperparameters and just apply it to the Cat dataset, so there is no need to iterate.
    \item If 100,000,000 examples is enough to build a good enough Cat detector, you might be better of training with just 10,000,000 examples to gain a $\approx$ 10x improvement in how quickly you can run experiments, even if each model performs a bit worse because it’s trained on less data. (X)
\end{itemize}